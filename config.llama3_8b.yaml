model: "llama3.1:8b-instruct-q8_0"

sampling:
  temperature: 0.0
  max_tokens: 64
  stream: false

run:
  seed: 7
  dataset: "data/xstest_full.csv"
  output_dir: "reports"
  run_name: "ans_llama3_8b"
  parallelism: 8

scoring:
  default: "regex"
  json_schema: null

perf:
  enabled: true
  lengths: [20, 200, 1000]
