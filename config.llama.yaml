model: "llama3.2:latest"

sampling:
  temperature: 0.0
  max_tokens: 512
  stream: false

run:
  seed: 7
  dataset: "data/boolq_smoke.csv"
  output_dir: "reports"
  run_name: "llama32_ollama"
  parallelism: 8

scoring:
  default: "regex"
  json_schema: null

perf:
  enabled: true
  lengths: [20, 200, 1000]
